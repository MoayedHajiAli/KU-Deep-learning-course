{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading trainging and testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install packages\n",
    "using Pkg\n",
    "haskey(Pkg.installed(),\"Knet\") || Pkg.add(\"Knet\")\n",
    "haskey(Pkg.installed(),\"CodecZlib\") || Pkg.add(\"CodecZlib\")\n",
    "haskey(Pkg.installed(),\"Images\") || Pkg.add(\"Images\")\n",
    "haskey(Pkg.installed(), \"Colors\") || Pkg.add(\"Colors\")\n",
    "haskey(Pkg.installed(), \"Statistics\") || Pkg.add(\"Statistics\")\n",
    "haskey(Pkg.installed(), \"Random\") || Pkg.add(\"Random\")\n",
    "using Knet\n",
    "using CodecZlib\n",
    "using Plots, Statistics, LinearAlgebra, Random\n",
    "using Images, ImageIO, QuartzImageIO, ImageMagick, Colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading train-images-idx3-ubyte.gz\n",
      "downloading train-labels-idx1-ubyte.gz\n",
      "downloading t10k-images-idx3-ubyte.gz\n",
      "downloading t10k-labels-idx1-ubyte.gz\n",
      "60008 10008\n",
      "60000 10000\n"
     ]
    }
   ],
   "source": [
    "# the data will be imported from \"http://yann.lecun.com/exdb/mnist\"\n",
    "mnsitUrl = \"http://yann.lecun.com/exdb/mnist\"\n",
    "mnsitDir = \"./Data\"\n",
    "xtrn_file, ytrn_file, xtst_file, ytst_file = \n",
    "    \"train-images-idx3-ubyte.gz\", \"train-labels-idx1-ubyte.gz\", \"t10k-images-idx3-ubyte.gz\", \"t10k-labels-idx1-ubyte.gz\"\n",
    "\n",
    "\n",
    "#download the traing and testing files\n",
    "function _mnsit_download_data(file)\n",
    "    println(\"downloading $file\")\n",
    "    if(!isdir(mnsitDir))\n",
    "        mkpath(mnsitDir)\n",
    "    end\n",
    "    filePath = joinpath(mnsitDir, file)\n",
    "    if(!isfile(filePath))\n",
    "        url = string(mnsitUrl, \"/\", file)\n",
    "        println(url)\n",
    "        download(url, filePath)\n",
    "    end\n",
    "    \n",
    "    #each images x file conatins the images represented as 784 pixels, which are normalized to [0-1] float numbers\n",
    "    #each label y is represented as a single number which represents the correct digit. 10 is used to represnt 0\n",
    "    f = GzipDecompressorStream(open(filePath))\n",
    "    a = read(f)\n",
    "    close(f)\n",
    "    return(a)\n",
    "end\n",
    "\n",
    "\n",
    "function _mnsit_load_data()\n",
    "    global xtrn, ytrn, xtst, ytst\n",
    "    xtrn, ytrn, xtst, ytst = _mnsit_download_data.([xtrn_file, ytrn_file, xtst_file, ytst_file])\n",
    "    #reshape y data to an array of dimantions 28*28*1*N, where N is the number of samples\n",
    "    _x_reshape(x) = reshape(x[17:end] ./ 255f0, (28,28,1,(length(x)÷(28*28))))\n",
    "    xtrn, xtst = _x_reshape.([xtrn, xtst])\n",
    "    println(length(ytrn), \" \" , length(ytst))\n",
    "    _y_reshape(y) = (Int.(y);(y[y.==0] .= 10); y[9:end])\n",
    "    ytrn, ytst = _y_reshape.([ytrn, ytst])\n",
    "    println(length(ytrn), \" \" , length(ytst))\n",
    "end\n",
    "\n",
    "_mnsit_load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show couple of samples from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "28×28 Array{Gray{Float32},2} with eltype Gray{Float32}:\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " ⋮                                       ⋱                    \n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)  …  Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)\n",
       " Gray{Float32}(0.0)  Gray{Float32}(0.0)     Gray{Float32}(0.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The correct digit is: 6\n"
     ]
    }
   ],
   "source": [
    "ind = rand(1:size(xtrn, 4))\n",
    "img = xtrn[:,:,1,ind]\n",
    "println(\"The correct digit is: \", ytrn[ind])\n",
    "Image = Gray.(img)\n",
    "display(Image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the data and essential functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting y data to one hot represntation\n",
    "function one_hot(y)\n",
    "    tmp = zeros(10,length(y))\n",
    "    for i in 1:length(y)\n",
    "        tmp[y[i], i] = 1\n",
    "    end\n",
    "    return tmp\n",
    "end\n",
    "ytrn, ytst = one_hot.([ytrn, ytst])\n",
    "#converting x data into 784*N matrix\n",
    "xtrn = Array{Float32}(mat(xtrn))\n",
    "xtst = Array{Float32}(mat(xtst))\n",
    "nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784×60000 Array{Float32,2}\n",
      "10×60000 Array{Float64,2}\n",
      "784×10000 Array{Float32,2}\n",
      "10×10000 Array{Float64,2}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "println.(summary.((xtrn, ytrn, xtst, ytst)))\n",
    "#divid the data into minibatches\n",
    "dtrn = minibatch(xtrn,ytrn,100)\n",
    "dtst = minibatch(xtst,ytst,100)\n",
    "#define accuracy\n",
    "function accuracy(w, data)\n",
    "    mean = 0\n",
    "    for (x,y) in data\n",
    "        for i in size(y, 2)\n",
    "            mean += argmax(w*x[:,i]) == argmax(y[:,i])\n",
    "        end\n",
    "    end\n",
    "    return mean/length(data)\n",
    "end\n",
    "#define W, our predection parameters\n",
    "w = rand(Float32, (10, 784))\n",
    "#accuracy test with random parameters\n",
    "accuracy(w, dtst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large VS Small batch-size training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "syntax: unexpected \"end\"",
     "output_type": "error",
     "traceback": [
      "syntax: unexpected \"end\"",
      ""
     ]
    }
   ],
   "source": [
    "#trainging using large batch size (100)\n",
    "function train(f, data, iter)\n",
    "    w = rand(Float32, (10, 784))\n",
    "    println(length(data))\n",
    "    for i in 1:iter\n",
    "        for (x,y) in data\n",
    "            f(w,x,y)\n",
    "        end\n",
    "        println(\"iteration $i has an accuracy in the dtst of $(accuracy(w,dtst))\", \" \", norm(w))\n",
    "        println(\"iteration $i has an accuracy in the dtrn of $(accuracy(w,dtrn))\", \" \", norm(w))\n",
    "        end\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#trainging using small batch size (1)\n",
    "function train(f, xtrn::Array, ytrn::Array, iter)\n",
    "    w = rand(Float32, (10, 784))\n",
    "    for i in 1:(1<<iter)\n",
    "        last = rand(1:size(xtrn,2))\n",
    "        xx,yy = xtrn[:,last:last], ytrn[:,last:last]\n",
    "        f(w,xx,yy)\n",
    "        if log2(i) ≈ round(log2(i))\n",
    "            println(\"iteration $i has an accuracy in the dtst of $(accuracy(w,dtst))\", \" \", norm(w))\n",
    "            println(\"iteration $i has an accuracy in the dtrn of $(accuracy(w,dtrn))\", \" \", norm(w))\n",
    "        end\n",
    "    end\n",
    "    println(\"iteration (1<<iter) has an accuracy in the dtst of $(accuracy(w,dtst))\", \" \", norm(w))\n",
    "    println(\"iteration (1<<iter) has an accuracy in the dtrn of $(accuracy(w,dtrn))\", \" \", norm(w))\n",
    "    return w\n",
    "end\n",
    "#=note that will larger batch size we achieve higher accuracy on the training dataset, but lower accurcy in the testing\n",
    "dataset, and vica versa with lower batch size=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Layer Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function perceptron(w,x,y)\n",
    "    pred = w*x\n",
    "    for i in size(y,2)\n",
    "        guess = argmax(pred[:,i])\n",
    "        class = argmax(y[:,i])\n",
    "        if guess != class\n",
    "            w[guess,:] -= x[:,i]\n",
    "            w[class,:] += x[:,i]\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"One layer Perceptron with batch size of 100\")\n",
    "wperceptron_1 = train(perceptron, xtrn, ytrn, 20)\n",
    "println(\"One layer Perceptron with batch size of 1\")\n",
    "wperceptron_100 = train(perceptron, dtrn, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function adaline(w, x, y; lr = 1e-4)\n",
    "    err = w * x - y\n",
    "    w .-= lr * err * x'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Adaline with batch size of 100\")\n",
    "wAdaline_1 = train(adaline, xtrn, ytrn, 20)\n",
    "println(\"Adaline with batch size of 1\")\n",
    "wAdaline_100 = train(adaline, dtrn, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function softMax(w, x, y, lr = 0.000001)\n",
    "    probs = exp.(w*x)\n",
    "    println(summary(probs))\n",
    "    probs ./= sum(probs)\n",
    "    err = props - y\n",
    "    w .-= lr * err * x'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"SoftMax with batch size of 100\")\n",
    "wSoftmax_1 = train(softMax, xtrn, ytrn, 20)\n",
    "println(\"SoftMax with batch size of 1\")\n",
    "wSoftmax_100 = train(softMax, dtrn, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize (generic function with 2 methods)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optemize with an array data set\n",
    "#define argmax and max for Knet types\n",
    "Base.argmax(a::KnetArray) = argmax(Array(a))\n",
    "Base.argmax(a::AutoGrad.Value) = argmax(value(a))\n",
    "Base.max(a::KnetArray) = max(Array(a))\n",
    "Base.max(a::AutoGrad.Value) = max(value(a))\n",
    "ARRAY = Array{Float32}\n",
    "function optimize(loss, x::Array , y::Array, iter; lr = 1e-4)\n",
    "    #specify that the elements of w are paprameters(meaning you want the gradient to be with respect to them)\n",
    "    w = Param(rand(Float32, (size(y,1), size(x,1))))\n",
    "    for i in 1:(1<<iter)\n",
    "        #choose a random data entry\n",
    "        ind = rand(1:size(x,2))\n",
    "        L = @diff loss(w, x[:,ind], y[:,ind])\n",
    "        ∇w = grad(L, w)\n",
    "        w .-= lr * ∇w\n",
    "        if log2(i) ≈ round(log2(i))\n",
    "            println(\"iteration $i has an accuracy in the dtst of $(accuracy(w,dtst))\", \" \", norm(w))\n",
    "            println(\"iteration $i has an accuracy in the dtrn of $(accuracy(w,dtrn))\", \" \", norm(w))\n",
    "        end\n",
    "    end\n",
    "    println(\"iteration $((1<<iter)) has an accuracy in the dtst of $(accuracy(w,dtst))\", \" \", norm(w))\n",
    "    println(\"iteration $((1<<iter))has an accuracy in the dtrn of $(accuracy(w,dtrn))\", \" \", norm(w))\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "optimize (generic function with 2 methods)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#optemize with an array data set (batch size > 1)\n",
    "#define argmax and max for Knet types\n",
    "Base.argmax(a::KnetArray) = argmax(Array(a))\n",
    "Base.argmax(a::AutoGrad.Value) = argmax(value(a))\n",
    "Base.max(a::KnetArray) = max(Array(a))\n",
    "Base.max(a::AutoGrad.Value) = max(value(a))\n",
    "function optimize(loss, data, iter; lr = 1e-4)\n",
    "    #specify that the elements of w are paprameters(meaning you want the gradient to be with respect to them)\n",
    "    (x,y) = first(data)\n",
    "    w = Param(rand(Float32, (size(y,1), size(x,1))))\n",
    "    for i in 1:iter\n",
    "        for (x,y) in data\n",
    "            L = @diff loss(w,x,y)\n",
    "            ∇w = grad(L, w)\n",
    "            w .-= lr * ∇w\n",
    "        end\n",
    "        println(\"iteration $i has an accuracy in the dtst of $(accuracy(w,dtst))\", \" \", norm(w))\n",
    "        println(\"iteration $i has an accuracy in the dtrn of $(accuracy(w,dtrn))\", \" \", norm(w))\n",
    "    end\n",
    "    return w\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron loss for Mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perceptronLoss (generic function with 1 method)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function perceptronLoss(w, x, y)\n",
    "    ŷ = w * x\n",
    "    #println.(summary.([w,x,y,ŷ]))\n",
    "    return sum([ŷ[argmax(ŷ[:,i])] - ŷ[argmax(y[:,i])] for i in size(y,2)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 has an accuracy in the dtst of 0.08 50.766262\n",
      "iteration 1 has an accuracy in the dtrn of 0.12333333333333334 50.766262\n",
      "iteration 2 has an accuracy in the dtst of 0.08 50.766254\n",
      "iteration 2 has an accuracy in the dtrn of 0.12333333333333334 50.766254\n",
      "iteration 4 has an accuracy in the dtst of 0.08 50.766254\n",
      "iteration 4 has an accuracy in the dtrn of 0.12333333333333334 50.766254\n",
      "iteration 8 has an accuracy in the dtst of 0.08 50.766247\n",
      "iteration 8 has an accuracy in the dtrn of 0.12333333333333334 50.766247\n",
      "iteration 16 has an accuracy in the dtst of 0.08 50.766262\n",
      "iteration 16 has an accuracy in the dtrn of 0.12333333333333334 50.766262\n",
      "iteration 32 has an accuracy in the dtst of 0.08 50.766205\n",
      "iteration 32 has an accuracy in the dtrn of 0.12166666666666667 50.766205\n",
      "iteration 64 has an accuracy in the dtst of 0.08 50.766117\n",
      "iteration 64 has an accuracy in the dtrn of 0.12166666666666667 50.766117\n",
      "iteration 128 has an accuracy in the dtst of 0.07 50.766045\n",
      "iteration 128 has an accuracy in the dtrn of 0.12 50.766045\n",
      "iteration 256 has an accuracy in the dtst of 0.08 50.765858\n",
      "iteration 256 has an accuracy in the dtrn of 0.12 50.765858\n",
      "iteration 512 has an accuracy in the dtst of 0.06 50.76542\n",
      "iteration 512 has an accuracy in the dtrn of 0.12 50.76542\n",
      "iteration 1024 has an accuracy in the dtst of 0.08 50.764824\n",
      "iteration 1024 has an accuracy in the dtrn of 0.12 50.764824\n",
      "iteration 1024 has an accuracy in the dtst of 0.08 50.764824\n",
      "iteration 1024 has an accuracy in the dtrn of 0.12 50.764824\n",
      "iteration 1 has an accuracy in the dtst of 0.08 233.80815\n",
      "iteration 1 has an accuracy in the dtrn of 0.17833333333333334 233.80815\n",
      "iteration 2 has an accuracy in the dtst of 0.04 414.25143\n",
      "iteration 2 has an accuracy in the dtrn of 0.10333333333333333 414.25143\n",
      "iteration 3 has an accuracy in the dtst of 0.03 605.75635\n",
      "iteration 3 has an accuracy in the dtrn of 0.095 605.75635\n",
      "iteration 4 has an accuracy in the dtst of 0.07 800.53986\n",
      "iteration 4 has an accuracy in the dtrn of 0.11 800.53986\n",
      "iteration 5 has an accuracy in the dtst of 0.08 999.39215\n",
      "iteration 5 has an accuracy in the dtrn of 0.12166666666666667 999.39215\n",
      "iteration 6 has an accuracy in the dtst of 0.07 1190.6964\n",
      "iteration 6 has an accuracy in the dtrn of 0.14166666666666666 1190.6964\n",
      "iteration 7 has an accuracy in the dtst of 0.08 1384.537\n",
      "iteration 7 has an accuracy in the dtrn of 0.15 1384.537\n",
      "\n",
      "Stacktrace:\n",
      " [1] \u001b[1mforwargs\u001b[22m\u001b[1m(\u001b[22m::Function, ::Tuple{Param{Array{Float32,2}},Array{Float32,2}}\u001b[1m)\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:107\u001b[22m\n",
      " [2] \u001b[1m#forw#1\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(AutoGrad.forw), ::Function, ::Param{Array{Float32,2}}, ::Vararg{Any,N} where N\u001b[1m)\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:65\u001b[22m\n",
      " [3] \u001b[1mforw\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:65\u001b[22m [inlined]\n",
      " [4] \u001b[1m*\u001b[22m at \u001b[1m./none:0\u001b[22m [inlined]\n",
      " [5] \u001b[1mperceptronLoss\u001b[22m\u001b[1m(\u001b[22m::Param{Array{Float32,2}}, ::Array{Float32,2}, ::Array{Float64,2}\u001b[1m)\u001b[22m at \u001b[1m./In[33]:2\u001b[22m\n",
      " [6] \u001b[1m(::var\"#55#56\"{typeof(perceptronLoss),Param{Array{Float32,2}},Array{Float32,2},Array{Float64,2}})\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:205\u001b[22m\n",
      " [7] \u001b[1m#differentiate#3\u001b[22m\u001b[1m(\u001b[22m::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(AutoGrad.differentiate), ::Function\u001b[1m)\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:144\u001b[22m\n",
      " [8] \u001b[1mdifferentiate\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:135\u001b[22m [inlined]\n",
      " [9] \u001b[1m#optimize#54\u001b[22m\u001b[1m(\u001b[22m::Int64, ::typeof(optimize), ::typeof(perceptronLoss), ::Knet.Data{Tuple{Array{Float32,2},Array{Float64,2}}}, ::Int64\u001b[1m)\u001b[22m at \u001b[1m./In[38]:13\u001b[22m\n",
      " [10] \u001b[1m(::var\"#kw##optimize\")\u001b[22m\u001b[1m(\u001b[22m::NamedTuple{(:lr,),Tuple{Int64}}, ::typeof(optimize), ::Function, ::Knet.Data{Tuple{Array{Float32,2},Array{Float64,2}}}, ::Int64\u001b[1m)\u001b[22m at \u001b[1m./none:0\u001b[22m\n",
      " [11] top-level scope at \u001b[1mIn[42]:2\u001b[22m\n",
      " [12] \u001b[1meval\u001b[22m at \u001b[1m./boot.jl:330\u001b[22m [inlined]\n",
      " [13] \u001b[1msoftscope_include_string\u001b[22m\u001b[1m(\u001b[22m::Module, ::String, ::String\u001b[1m)\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/SoftGlobalScope/cSbw5/src/SoftGlobalScope.jl:218\u001b[22m\n",
      " [14] \u001b[1mexecute_request\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket, ::IJulia.Msg\u001b[1m)\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/IJulia/yLI42/src/execute_request.jl:67\u001b[22m\n",
      " [15] \u001b[1m#invokelatest#1\u001b[22m at \u001b[1m./essentials.jl:709\u001b[22m [inlined]\n",
      " [16] \u001b[1minvokelatest\u001b[22m at \u001b[1m./essentials.jl:708\u001b[22m [inlined]\n",
      " [17] \u001b[1meventloop\u001b[22m\u001b[1m(\u001b[22m::ZMQ.Socket\u001b[1m)\u001b[22m at \u001b[1m/Users/moaid/.julia/packages/IJulia/yLI42/src/eventloop.jl:8\u001b[22m\n",
      " [18] \u001b[1m(::IJulia.var\"#15#18\")\u001b[22m\u001b[1m(\u001b[22m\u001b[1m)\u001b[22m at \u001b[1m./task.jl:333\u001b[22m\n"
     ]
    },
    {
     "ename": "InterruptException",
     "evalue": "InterruptException:",
     "output_type": "error",
     "traceback": [
      "InterruptException:",
      "",
      "Stacktrace:",
      " [1] #differentiate#3(::Base.Iterators.Pairs{Union{},Union{},Tuple{},NamedTuple{(),Tuple{}}}, ::typeof(AutoGrad.differentiate), ::Function) at /Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:148",
      " [2] differentiate at /Users/moaid/.julia/packages/AutoGrad/FSgUc/src/core.jl:135 [inlined]",
      " [3] #optimize#54(::Int64, ::typeof(optimize), ::typeof(perceptronLoss), ::Knet.Data{Tuple{Array{Float32,2},Array{Float64,2}}}, ::Int64) at ./In[38]:13",
      " [4] (::var\"#kw##optimize\")(::NamedTuple{(:lr,),Tuple{Int64}}, ::typeof(optimize), ::Function, ::Knet.Data{Tuple{Array{Float32,2},Array{Float64,2}}}, ::Int64) at ./none:0",
      " [5] top-level scope at In[42]:2"
     ]
    }
   ],
   "source": [
    "wperceptron2_1 = optimize(perceptronLoss, xtrn, ytrn,10)\n",
    "wperceptron2_100 = optimize(perceptronLoss, dtrn, 100, lr = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
