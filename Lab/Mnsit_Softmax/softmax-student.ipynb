{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifying MNIST digits with a softmax classifier\n",
    "# Name: Moayed Haji Ali"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will implement a softmax classifier to predict the digit presented in a given image. We will use the MNIST dataset for this task. Please first skim through the notebook. Then complete the following steps mentioned in the main function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. minibatch\n",
    "2. init_params\n",
    "3. forward and backward propagation\n",
    "    * softmax_forw\n",
    "    * softmax_back_and_loss\n",
    "4. grad_check\n",
    "5. train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; for p in [\"Knet\"]; haskey(Pkg.installed(),p) || Pkg.add(p); end #Knet installation to use the MNIST dataset\n",
    "using Knet, Printf, Random\n",
    "import Knet: minibatch, accuracy\n",
    "include(Knet.dir(\"data\", \"mnist.jl\"))\n",
    "using Printf, Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "minibatch (generic function with 2 methods)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takes raw input (X) and gold labels (Y)\n",
    "# returns list of minibatches (x, y)\n",
    "function minibatch(X, Y, bs=100)\n",
    "    data = Any[]\n",
    "    i = 1\n",
    "    while i <= size(X,2)\n",
    "        push!(data, [X[:, i:min(end,i+bs-1)], Y[:, i:min(end, i+bs-1)]])\n",
    "        i += bs\n",
    "    end\n",
    "    return data\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_params (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init_params(ninputs, noutputs)\n",
    "# this functions takes two arguments,\n",
    "#   number of input features (ninputs::Int)\n",
    "#   and number of output classes (noutputs::Int)\n",
    "# returns randomly generated W (sampled from N(0,1e-3))\n",
    "#   and b(must be zeros vector) params of softmax\n",
    "function init_params(ninputs, noutputs)\n",
    "    w = Param(rand(Float32, (noutputs, ninputs)) * 0.001)\n",
    "    b = Param(Array{Float64}(zeros(noutputs, 1)))\n",
    "    return w,b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_forw (generic function with 1 method)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax_forw(W, b, x)\n",
    "# this function takes three arguments,\n",
    "#   model weights (W,b) and single minibatch input data (x)\n",
    "# applies the affine transformation and softmax function\n",
    "# returns predicted probabilities\n",
    "function softmax_forw(W, b, x)\n",
    "    probs = exp.(W * x .+ b)\n",
    "    probs ./= sum(probs, dims = 1)\n",
    "    return probs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "softmax_back_and_loss (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# softmax_back_and_loss(W, b, x, ygold)\n",
    "# takes model weights (W,b), input data (x) and correct labels (ygold)\n",
    "# calculates and returns the soft loss, gradients of w and b\n",
    "function softmax_back_and_loss(W, b, x, ygold)\n",
    "    function cost(W, b, x, ygold)\n",
    "        probs = softmax_forw(W, b, x)\n",
    "        class = argmax(ygold, dims = 1)\n",
    "        mean(-log.(probs[class])) \n",
    "    end\n",
    "    probs = softmax_forw(W, b, x)\n",
    "    d = (probs .- ygold) / size(x,2)\n",
    "    ∇b = sum(d, dims=2)    # gradient of the cost wrt b\n",
    "    ∇W = d * x'         # gradient of the cost wrt W\n",
    "    return cost(W, b, x, ygold), ∇W, ∇b \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "grad_check (generic function with 1 method)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# grad_check(W, b, x, ygold)\n",
    "# takes model weights (W,b), one minibatch input data (x) and\n",
    "#   and true labels (ygold) as input parameters\n",
    "# displays info about whether your gradient calculation procedure\n",
    "#   passes the gradient check test or not.\n",
    "function grad_check(W, b, x, ygold)\n",
    "    #diminsions:\n",
    "    # W (out, features) b (out, 1) x(featurs, instancs), ygold(out, instances)\n",
    "    # numeric_gradient()\n",
    "    # calculates and returns numeric gradients of model weights (gw,gb)\n",
    "    function numeric_gradient()\n",
    "        epsilon = 0.0001\n",
    "        cost(W,b,x,ygold) = softmax_back_and_loss(W,b,x,ygold)[1]\n",
    "        gw = zeros(size(W)) # gradient of W\n",
    "        gb = zeros(size(b)) # gradient of b\n",
    "        R = CartesianIndices((1:size(W,1),1:size(W,2)))\n",
    "        for i in R\n",
    "            W[i] -= epsilon\n",
    "            prevcost,_,_ = softmax_back_and_loss(W,b,x,ygold)\n",
    "            W[i] += 2*epsilon\n",
    "            aftercost,_,_ = softmax_back_and_loss(W,b,x,ygold)\n",
    "            gw[i] = (aftercost-prevcost)/(2*epsilon)\n",
    "            W[i] -= epsilon\n",
    "        end\n",
    "        for i in 1:size(b,1)\n",
    "            b[i] -= epsilon\n",
    "            prevcost = cost(W, b, x, ygold)\n",
    "            b[i] += 2*epsilon\n",
    "            aftercost = cost(W, b, x, ygold)\n",
    "            gb[i] = (aftercost-prevcost)/(2*epsilon)\n",
    "            b[i] -= epsilon\n",
    "        end\n",
    "        return gw, gb\n",
    "    end\n",
    "    x = Array{Float64}(x)\n",
    "    println.(summary.((W, b, x, ygold)))\n",
    "    _,gradW,gradB = softmax_back_and_loss(W, b, x, ygold)\n",
    "    println(\"Finished\")\n",
    "    gw, gb = numeric_gradient()\n",
    "\n",
    "    diff = sqrt(sum((gradW - gw) .^ 2) + sum((gradB - gb) .^ 2))\n",
    "    println(\"Diff: $diff\")\n",
    "    if diff < 1e-7\n",
    "        println(\"Gradient Checking Passed\")\n",
    "    else\n",
    "        println(\"Diff must be < 1e-7\")\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train (generic function with 2 methods)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train(W, b, data, lr=0.15)\n",
    "# takes model weights (W,b), data split (data) and learning rate (lr) as input\n",
    "# iterates over the whole data split and in each iteration network is updated\n",
    "# by using the gradients obtained from softmax_back_and_loss function call. default learning\n",
    "# rate is 0.15. it returns the average softloss over the whole data split.\n",
    "function train(W, b, data, lr=0.15)\n",
    "    totalcost = 0.0\n",
    "    numins = 0\n",
    "    for (x, y) in data\n",
    "        ls, gw, gb = softmax_back_and_loss(W, b, x, y)\n",
    "        totalcost += ls*size(x,2)\n",
    "        numins += size(x,2)\n",
    "        W .-= lr * gw\n",
    "        b .-= lr * gb\n",
    "        #println.(summary.((W, b, w2, b2)))\n",
    "    end\n",
    "\n",
    "    avgcost = totalcost / numins\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy (generic function with 4 methods)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't touch this cell. Read it carefully.\n",
    "# accuracy(ygold, ypred)\n",
    "# takes true labels (ygold) and predicted scores as input for single minibatch.\n",
    "# returns accuracy\n",
    "function accuracy(ygold, ypred)\n",
    "    correct = 0.0\n",
    "    for i=1:size(ygold, 2)\n",
    "        correct += findmax(ygold[:,i]; dims=1)[2] == findmax(ypred[:, i]; dims=1)[2] ? 1.0 : 0.0\n",
    "    end\n",
    "    return correct / size(ygold, 2)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "main (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Don't touch this cell. Read it carefully.\n",
    "#load mnist data\n",
    "function main()\n",
    "    Random.seed!(12345)\n",
    "\n",
    "    # Size of input vector (MNIST images are 28x28)\n",
    "    ninputs = 28 * 28\n",
    "\n",
    "    # Number of classes (MNIST images fall into 10 classes)\n",
    "    noutputs = 10\n",
    "    \n",
    "    ### Data loading & preprocessing\n",
    "    xtrn, ytrn, xtst, ytst = mnist(); # loading the data\n",
    "    println(\"Finished loading\")\n",
    "    #  In this section, we load the input and output data,\n",
    "    #  prepare data to feed into softmax model.\n",
    "    #  For softmax regression on MNIST pixels,\n",
    "    #  the input data is the images, and\n",
    "    #  the output data is the labels.\n",
    "    #  Size of xtrn: (28,28,1,60000)\n",
    "    #  Size of xtrn must be: (784, 60000)\n",
    "    #  Size of xtst must be: (784, 10000)\n",
    "    #  Size of ytrn must be: (10, 60000)\n",
    "    #  Size of ytst must be: (10, 10000)\n",
    "\n",
    "    xtrn = reshape(xtrn, 784, 60000)\n",
    "    xtst = reshape(xtst, 784, 10000)\n",
    "\n",
    "    function to_onehot(x)\n",
    "        onehot = zeros(10, 1)\n",
    "        onehot[x, 1] = 1.0\n",
    "        return onehot\n",
    "    end\n",
    "\n",
    "    ytrn = hcat(map(to_onehot, ytrn)...)\n",
    "    ytst = hcat(map(to_onehot, ytst)...)\n",
    "\n",
    "    # STEP 1: Create minibatches\n",
    "    #   Complete the minibatch function\n",
    "    #   It takes the input matrix (X) and gold labels (Y)\n",
    "    #   returns list of tuples contain minibatched input and labels (x, y)\n",
    "    bs = 100\n",
    "    println(\"Strated minibatching\")\n",
    "    trn_data = minibatch(xtrn, ytrn, bs)\n",
    "    \n",
    "    println(\"finished minibatching\")\n",
    "\n",
    "    # STEP 2: Initialize parameters\n",
    "    #   Complete init_params function\n",
    "    #   It takes number of inputs and number of outputs(number of classes)\n",
    "    #   It returns randomly generated W matrix and bias vector\n",
    "    #   Sample from N(0, 0.001)\n",
    "\n",
    "    W, b = init_params(ninputs, noutputs)\n",
    "    # STEP 3: Implement softmax_forw and softmax_back_and_loss\n",
    "    #   softmax_forw function takes W, b, and data\n",
    "    #   calculates predicted probabilities\n",
    "    #\n",
    "    #   softmax_back_and_loss function obtains probabilites by calling\n",
    "    #   softmax_forw then calculates soft loss and gradients of W and b\n",
    "\n",
    "    # STEP 4: Gradient checking\n",
    "    #   Skip this part for the lab session.\n",
    "    #   As with any learning algorithm, you should always check that your\n",
    "    #   gradients are correct before learning the parameters.\n",
    "\n",
    "    debug = true # Turn this parameter off, after gradient checking passed\n",
    "    if debug\n",
    "        grad_check(W, b, xtrn[:, 1:100], ytrn[:, 1:100])\n",
    "    end\n",
    "\n",
    "    lr = 0.15\n",
    "\n",
    "    # STEP 5: Training\n",
    "    #   The train function takes model parameters and the data\n",
    "    #   Trains the model over minibatches\n",
    "    #   For each minibatch, first cost and gradients are calculated then model parameters are updated\n",
    "    #   train function returns the average cost per instance\n",
    "\n",
    "    for i=1:50\n",
    "        cost = train(W, b, trn_data, lr)\n",
    "        pred = softmax_forw(W, b, xtrn)\n",
    "        trnacc = accuracy(ytrn, pred)\n",
    "        pred = softmax_forw(W, b, xtst)\n",
    "        tstacc = accuracy(ytst, pred)\n",
    "        @printf(\"epoch: %d softloss: %g trn accuracy: %g tst accuracy: %g\\n\", i, cost, trnacc, tstacc)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading\n",
      "Strated minibatching\n",
      "finished minibatching\n",
      "10×784 Param{Array{Float64,2}}\n",
      "10×1 Param{Array{Float64,2}}\n",
      "784×100 Array{Float64,2}\n",
      "10×100 Array{Float64,2}\n",
      "Finished\n",
      "Diff: 1.8305383451682588e-9\n",
      "Gradient Checking Passed\n",
      "epoch: 1 softloss: 0.481573 trn accuracy: 0.896983 tst accuracy: 0.906\n",
      "epoch: 2 softloss: 0.339108 trn accuracy: 0.9075 tst accuracy: 0.9119\n",
      "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9143\n",
      "epoch: 4 softloss: 0.303877 trn accuracy: 0.914767 tst accuracy: 0.9155\n",
      "epoch: 5 softloss: 0.29597 trn accuracy: 0.916633 tst accuracy: 0.9171\n",
      "epoch: 6 softloss: 0.290259 trn accuracy: 0.918067 tst accuracy: 0.9185\n",
      "epoch: 7 softloss: 0.285859 trn accuracy: 0.91925 tst accuracy: 0.9198\n",
      "epoch: 8 softloss: 0.282318 trn accuracy: 0.9201 tst accuracy: 0.92\n",
      "epoch: 9 softloss: 0.279379 trn accuracy: 0.920883 tst accuracy: 0.9206\n",
      "epoch: 10 softloss: 0.27688 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
      "epoch: 11 softloss: 0.274718 trn accuracy: 0.922283 tst accuracy: 0.9208\n",
      "epoch: 12 softloss: 0.272818 trn accuracy: 0.923 tst accuracy: 0.9213\n",
      "epoch: 13 softloss: 0.271128 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
      "epoch: 14 softloss: 0.269611 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
      "epoch: 15 softloss: 0.268236 trn accuracy: 0.9244 tst accuracy: 0.9219\n",
      "epoch: 16 softloss: 0.266982 trn accuracy: 0.924683 tst accuracy: 0.9218\n",
      "epoch: 17 softloss: 0.26583 trn accuracy: 0.924917 tst accuracy: 0.9219\n",
      "epoch: 18 softloss: 0.264766 trn accuracy: 0.9251 tst accuracy: 0.9221\n",
      "epoch: 19 softloss: 0.263778 trn accuracy: 0.925367 tst accuracy: 0.9224\n",
      "epoch: 20 softloss: 0.262858 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
      "epoch: 21 softloss: 0.261996 trn accuracy: 0.926283 tst accuracy: 0.9227\n",
      "epoch: 22 softloss: 0.261187 trn accuracy: 0.9266 tst accuracy: 0.9225\n",
      "epoch: 23 softloss: 0.260425 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
      "epoch: 24 softloss: 0.259705 trn accuracy: 0.92715 tst accuracy: 0.9228\n",
      "epoch: 25 softloss: 0.259023 trn accuracy: 0.927367 tst accuracy: 0.9226\n",
      "epoch: 26 softloss: 0.258376 trn accuracy: 0.927533 tst accuracy: 0.9228\n",
      "epoch: 27 softloss: 0.25776 trn accuracy: 0.927783 tst accuracy: 0.923\n",
      "epoch: 28 softloss: 0.257172 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
      "epoch: 29 softloss: 0.256611 trn accuracy: 0.92825 tst accuracy: 0.923\n",
      "epoch: 30 softloss: 0.256074 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
      "epoch: 31 softloss: 0.255559 trn accuracy: 0.928567 tst accuracy: 0.923\n",
      "epoch: 32 softloss: 0.255065 trn accuracy: 0.928767 tst accuracy: 0.9228\n",
      "epoch: 33 softloss: 0.254591 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
      "epoch: 34 softloss: 0.254134 trn accuracy: 0.929067 tst accuracy: 0.9227\n",
      "epoch: 35 softloss: 0.253694 trn accuracy: 0.929167 tst accuracy: 0.923\n",
      "epoch: 36 softloss: 0.253269 trn accuracy: 0.929267 tst accuracy: 0.9228\n",
      "epoch: 37 softloss: 0.252859 trn accuracy: 0.929383 tst accuracy: 0.9229\n",
      "epoch: 38 softloss: 0.252463 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
      "epoch: 39 softloss: 0.252079 trn accuracy: 0.92965 tst accuracy: 0.9228\n",
      "epoch: 40 softloss: 0.251708 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
      "epoch: 41 softloss: 0.251348 trn accuracy: 0.929883 tst accuracy: 0.9231\n",
      "epoch: 42 softloss: 0.250999 trn accuracy: 0.93005 tst accuracy: 0.9233\n",
      "epoch: 43 softloss: 0.250661 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
      "epoch: 44 softloss: 0.250332 trn accuracy: 0.930217 tst accuracy: 0.9236\n",
      "epoch: 45 softloss: 0.250012 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
      "epoch: 46 softloss: 0.249701 trn accuracy: 0.930517 tst accuracy: 0.9237\n",
      "epoch: 47 softloss: 0.249398 trn accuracy: 0.930567 tst accuracy: 0.9238\n",
      "epoch: 48 softloss: 0.249103 trn accuracy: 0.930717 tst accuracy: 0.9238\n",
      "epoch: 49 softloss: 0.248816 trn accuracy: 0.930867 tst accuracy: 0.9242\n",
      "epoch: 50 softloss: 0.248536 trn accuracy: 0.930967 tst accuracy: 0.9243\n"
     ]
    }
   ],
   "source": [
    "main()\n",
    "\n",
    "#= Example Output\n",
    "Diff: 1.8292339049184216e-9\n",
    "Gradient Checking Passed\n",
    "epoch: 1 softloss: 0.481559 trn accuracy: 0.896983 tst accuracy: 0.9064\n",
    "epoch: 2 softloss: 0.339105 trn accuracy: 0.907617 tst accuracy: 0.9119\n",
    "epoch: 3 softloss: 0.31604 trn accuracy: 0.912017 tst accuracy: 0.9142\n",
    "epoch: 4 softloss: 0.303876 trn accuracy: 0.914783 tst accuracy: 0.9156\n",
    "epoch: 5 softloss: 0.29597 trn accuracy: 0.916567 tst accuracy: 0.9172\n",
    "epoch: 6 softloss: 0.290259 trn accuracy: 0.918033 tst accuracy: 0.9187\n",
    "epoch: 7 softloss: 0.285858 trn accuracy: 0.919233 tst accuracy: 0.9198\n",
    "epoch: 8 softloss: 0.282317 trn accuracy: 0.920083 tst accuracy: 0.92\n",
    "epoch: 9 softloss: 0.279378 trn accuracy: 0.9209 tst accuracy: 0.9204\n",
    "epoch: 10 softloss: 0.276879 trn accuracy: 0.921717 tst accuracy: 0.9211\n",
    "epoch: 11 softloss: 0.274716 trn accuracy: 0.92225 tst accuracy: 0.9207\n",
    "epoch: 12 softloss: 0.272816 trn accuracy: 0.92305 tst accuracy: 0.9214\n",
    "epoch: 13 softloss: 0.271127 trn accuracy: 0.923667 tst accuracy: 0.9214\n",
    "epoch: 14 softloss: 0.269609 trn accuracy: 0.924133 tst accuracy: 0.9215\n",
    "epoch: 15 softloss: 0.268235 trn accuracy: 0.924417 tst accuracy: 0.922\n",
    "epoch: 16 softloss: 0.26698 trn accuracy: 0.9247 tst accuracy: 0.9219\n",
    "epoch: 17 softloss: 0.265828 trn accuracy: 0.924933 tst accuracy: 0.9218\n",
    "epoch: 18 softloss: 0.264764 trn accuracy: 0.92505 tst accuracy: 0.922\n",
    "epoch: 19 softloss: 0.263777 trn accuracy: 0.925367 tst accuracy: 0.9223\n",
    "epoch: 20 softloss: 0.262856 trn accuracy: 0.92575 tst accuracy: 0.9225\n",
    "epoch: 21 softloss: 0.261995 trn accuracy: 0.9263 tst accuracy: 0.9227\n",
    "epoch: 22 softloss: 0.261186 trn accuracy: 0.926567 tst accuracy: 0.9226\n",
    "epoch: 23 softloss: 0.260424 trn accuracy: 0.9269 tst accuracy: 0.9229\n",
    "epoch: 24 softloss: 0.259704 trn accuracy: 0.92715 tst accuracy: 0.9227\n",
    "epoch: 25 softloss: 0.259022 trn accuracy: 0.927367 tst accuracy: 0.9227\n",
    "epoch: 26 softloss: 0.258374 trn accuracy: 0.9275 tst accuracy: 0.9229\n",
    "epoch: 27 softloss: 0.257758 trn accuracy: 0.927767 tst accuracy: 0.923\n",
    "epoch: 28 softloss: 0.257171 trn accuracy: 0.928083 tst accuracy: 0.9229\n",
    "epoch: 29 softloss: 0.25661 trn accuracy: 0.92825 tst accuracy: 0.9231\n",
    "epoch: 30 softloss: 0.256073 trn accuracy: 0.92835 tst accuracy: 0.9229\n",
    "epoch: 31 softloss: 0.255558 trn accuracy: 0.928517 tst accuracy: 0.923\n",
    "epoch: 32 softloss: 0.255064 trn accuracy: 0.928783 tst accuracy: 0.9228\n",
    "epoch: 33 softloss: 0.254589 trn accuracy: 0.92895 tst accuracy: 0.9229\n",
    "epoch: 34 softloss: 0.254133 trn accuracy: 0.9291 tst accuracy: 0.9227\n",
    "epoch: 35 softloss: 0.253692 trn accuracy: 0.929167 tst accuracy: 0.9228\n",
    "epoch: 36 softloss: 0.253268 trn accuracy: 0.92925 tst accuracy: 0.9227\n",
    "epoch: 37 softloss: 0.252858 trn accuracy: 0.929417 tst accuracy: 0.923\n",
    "epoch: 38 softloss: 0.252462 trn accuracy: 0.929567 tst accuracy: 0.9229\n",
    "epoch: 39 softloss: 0.252078 trn accuracy: 0.929667 tst accuracy: 0.9228\n",
    "epoch: 40 softloss: 0.251707 trn accuracy: 0.929783 tst accuracy: 0.9229\n",
    "epoch: 41 softloss: 0.251347 trn accuracy: 0.929867 tst accuracy: 0.9231\n",
    "epoch: 42 softloss: 0.250998 trn accuracy: 0.930067 tst accuracy: 0.9235\n",
    "epoch: 43 softloss: 0.25066 trn accuracy: 0.9301 tst accuracy: 0.9235\n",
    "epoch: 44 softloss: 0.250331 trn accuracy: 0.930233 tst accuracy: 0.9235\n",
    "epoch: 45 softloss: 0.250011 trn accuracy: 0.930333 tst accuracy: 0.9235\n",
    "epoch: 46 softloss: 0.2497 trn accuracy: 0.9305 tst accuracy: 0.9237\n",
    "epoch: 47 softloss: 0.249397 trn accuracy: 0.930583 tst accuracy: 0.9238\n",
    "epoch: 48 softloss: 0.249102 trn accuracy: 0.9307 tst accuracy: 0.9239\n",
    "epoch: 49 softloss: 0.248815 trn accuracy: 0.93085 tst accuracy: 0.9242\n",
    "epoch: 50 softloss: 0.248535 trn accuracy: 0.930933 tst accuracy: 0.9243\n",
    "=#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
